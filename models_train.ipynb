{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117547e0",
   "metadata": {},
   "source": [
    "# Projeto Final - Aprendizagem de Máquina 2025-2\n",
    "**SAINT (Self-Attention and Intersample Transformer)**\n",
    "\n",
    "Este notebook implementa todos os requisitos do projeto final, da segunda parte da disciplina de Aprendizagem de Máquina.\n",
    "\n",
    "**Referência do SAINT:**\n",
    "https://github.com/Actis92/lit-saint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e1a23",
   "metadata": {},
   "source": [
    "## Instalando os pacotes\n",
    "\n",
    "É necessário utilizar o `python 3.8` para instalar o SAINT corretamente. Aqui são instalados os seguintes pacotes:\n",
    "- `lit-saint`: Implementação do modelo SAINT.\n",
    "- `openml`: Biblioteca para carregar os datasets do OpenML.\n",
    "- `autogluon`: Modelo de automl utilizado para comparação.\n",
    "- `lightgbm`, ``xgboost`` e ``catboost``: Modelos para comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338a48d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'asttokens'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%pip install spacy==3.7.0 spacy-legacy==3.0.12 pandas==1.5.3 openml==0.14.2 \\\n",
    "    lit-saint==0.4.1 lightgbm==3.3.5 xgboost==1.7.6 catboost==1.2.5 autogluon==0.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b51f1",
   "metadata": {},
   "source": [
    "## Configurando o ambiente\n",
    "\n",
    "Faz as seguintes alterações:\n",
    "1. Utiliza o estilo 'seaborn-v0_8-darkgrid' para os gráficos e 'husl' para a paleta de cores.\n",
    "2. Ignora avisos para manter o notebook limpo.\n",
    "3. Define uma semente aleatória para garantir reprodutibilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a108b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'asttokens'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "SEARCH_ITERATIONS = 20\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "N_JOBS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fcd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b627066",
   "metadata": {},
   "source": [
    "## Obtenção e preparação dos dados\n",
    "\n",
    "Carrega os 2 menores datasets do OpenML para tarefas de classificação e regressão (valor ajustável no código)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf12c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "\n",
    "OPENML_CC18_ID = 99\n",
    "NUM_DATASETS = 10\n",
    "\n",
    "suite = openml.study.get_suite(suite_id=OPENML_CC18_ID)\n",
    "datasets_df = openml.datasets.list_datasets(data_id=suite.data, output_format='dataframe')\n",
    "\n",
    "# Seleciona os menores datasets\n",
    "datasets_df_sorted = datasets_df.sort_values(by='NumberOfInstances')\n",
    "top_datasets = datasets_df_sorted.head(NUM_DATASETS)\n",
    "\n",
    "datasets_memory = {}\n",
    "for idx, row in top_datasets.iterrows():\n",
    "    dataset_id = row['did']\n",
    "    dataset_name = row['name']\n",
    "    print(f\"Fetching {dataset_name} (ID: {dataset_id}, Instances: {row['NumberOfInstances']})...\")\n",
    "    \n",
    "    try:\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=dataset.default_target_attribute,\n",
    "            dataset_format='dataframe'\n",
    "        )\n",
    "\n",
    "        if y is not None:\n",
    "            X['target'] = y\n",
    "            \n",
    "        datasets_memory[dataset_name] = X\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {dataset_name}: {e}\")\n",
    "\n",
    "print(f\"Done! {len(datasets_memory)} datasets available in 'datasets_memory' dictionary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0ea37",
   "metadata": {},
   "source": [
    "## Divisão dos dados em conjuntos de treinamento e teste\n",
    "\n",
    "Cada dataset é dividido em:\n",
    "- **Conjunto de treino**: 70%\n",
    "- **Conjunto de teste**: 30%\n",
    "- **Seed**: Definida para reprodutibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1804373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_splits = {}\n",
    "for dataset_name, df in datasets_memory.items():\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    train_test_splits[dataset_name] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test)\n",
    "    }\n",
    "    \n",
    "    print(f\"{dataset_name}:\")\n",
    "    print(f\"  Treino: {len(X_train)} amostras (70%)\")\n",
    "    print(f\"  Teste:  {len(X_test)} amostras (30%)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total de {len(train_test_splits)} datasets divididos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c024b",
   "metadata": {},
   "source": [
    "## Busca de Hiperparâmetros com Validação Cruzada\n",
    "\n",
    "Realiza a otimização de hiperparâmetros para os seguintes modelos usando RandomizedSearchCV com validação cruzada estratificada:\n",
    "- **SAINT (lit-saint)**: Dimensionalidade, número de cabeças de atenção, taxa de aprendizado (implementação pendente)\n",
    "- **LightGBM**: Número de folhas, profundidade, taxa de aprendizado\n",
    "- **CatBoost**: Profundidade, iterações, taxa de aprendizado\n",
    "- **XGBoost**: Profundidade, taxa de aprendizado, número de estimadores\n",
    "- **AutoGluon**: AutoML com validação cruzada\n",
    "\n",
    "> Os modelos clássicos salvam suas predições e metadados em `artifacts/<modelo>/<dataset_slug>/`. Nenhum objeto treinado é persistido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from artifact_utils import write_artifact_bundle, get_dataset_dir\n",
    "\n",
    "MODEL_KEY_MAP = {\n",
    "    'LightGBM': 'lightgbm',\n",
    "    'XGBoost': 'xgboost',\n",
    "    'CatBoost': 'catboost',\n",
    "    'SAINT': 'saint',\n",
    "    'AutoGluon': 'autogluon',\n",
    "}\n",
    "\n",
    "\n",
    "def artifact_exists(model_name: str, dataset_name: str) -> bool:\n",
    "    \"\"\"Check if metadata + predictions already exist for this dataset/model.\"\"\"\n",
    "\n",
    "    model_key = MODEL_KEY_MAP.get(model_name, model_name.lower())\n",
    "    dataset_dir = get_dataset_dir(model_key, dataset_name)\n",
    "    return (\n",
    "        dataset_dir.joinpath('metadata.json').exists()\n",
    "        and dataset_dir.joinpath('predictions.npz').exists()\n",
    "    )\n",
    "\n",
    "\n",
    "def artifact_output_dir(model_name: str, dataset_name: str) -> str:\n",
    "    model_key = MODEL_KEY_MAP.get(model_name, model_name.lower())\n",
    "    return str(get_dataset_dir(model_key, dataset_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc89660",
   "metadata": {},
   "source": [
    "### Definição do espaço amostral\n",
    "\n",
    "Define o espaço de busca de hiperparâmetros para os seguintes modelos:\n",
    "- **LightGBM**\n",
    "- **CatBoost**\n",
    "- **XGBoost**\n",
    "- **SAINT (lit-saint)**\n",
    "\n",
    "E define a função de preprocessamento dos dados e a função de busca de hiperparâmetros para os três primeiros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc89660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from pytorch_lightning import Trainer\n",
    "from joblib import parallel_backend\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "param_spaces_classifier = {\n",
    "    'LightGBM': {\n",
    "        'num_leaves': randint(20, 151),\n",
    "        'max_depth': randint(3, 16),\n",
    "        'learning_rate': loguniform(1e-2, 3e-1),\n",
    "        'n_estimators': randint(50, 401),\n",
    "        'min_child_samples': randint(5, 101),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'reg_alpha': loguniform(1e-8, 10),\n",
    "        'reg_lambda': loguniform(1e-8, 10)\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'max_depth': randint(3, 16),\n",
    "        'learning_rate': loguniform(1e-2, 3e-1),\n",
    "        'n_estimators': randint(50, 401),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'min_child_weight': randint(1, 11),\n",
    "        'gamma': loguniform(1e-8, 1.0),\n",
    "        'reg_alpha': loguniform(1e-8, 10),\n",
    "        'reg_lambda': loguniform(1e-8, 10)\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'depth': randint(3, 11),\n",
    "        'border_count': randint(32, 256),\n",
    "        'learning_rate': loguniform(1e-2, 3e-1),\n",
    "        'iterations': randint(50, 401),\n",
    "        'l2_leaf_reg': uniform(1, 9),\n",
    "        'random_strength': uniform(0, 10),\n",
    "        'bagging_temperature': uniform(0, 1)\n",
    "    },\n",
    "    \"Saint\": {\n",
    "        'depth': randint(3, 11),\n",
    "        'heads': randint(4, 9),\n",
    "        'dropout': uniform(0.0, 0.5),\n",
    "        'learning_rate': loguniform(1e-4, 1e-2),\n",
    "        'batch_size': randint(64, 257),\n",
    "        'epochs': randint(5, 21)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def run_classification_searches(X_train, y_train, cv_strategy, target_models=None):\n",
    "    results = {}\n",
    "    search_space = {\n",
    "        'LightGBM': (\n",
    "            lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1),\n",
    "            param_spaces_classifier['LightGBM'],\n",
    "            N_JOBS,\n",
    "            'accuracy'\n",
    "        ),\n",
    "        'XGBoost': (\n",
    "            xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss', use_label_encoder=False),\n",
    "            param_spaces_classifier['XGBoost'],\n",
    "            N_JOBS,\n",
    "            'accuracy'\n",
    "        ),\n",
    "        'CatBoost': (\n",
    "            CatBoostClassifier(random_state=RANDOM_STATE, verbose=0),\n",
    "            param_spaces_classifier['CatBoost'],\n",
    "            1,\n",
    "            'accuracy'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    target_set = set(target_models) if target_models else None\n",
    "\n",
    "    for model_name, (estimator, param_dist, n_jobs, scoring) in search_space.items():\n",
    "        if target_set and model_name not in target_set:\n",
    "            continue\n",
    "        print(f\"\\n  {model_name} (RandomizedSearchCV)...\", end=' ', flush=True)\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=estimator,\n",
    "                param_distributions=param_dist,\n",
    "                n_iter=SEARCH_ITERATIONS,\n",
    "                cv=cv_strategy,\n",
    "                n_jobs=n_jobs,\n",
    "                scoring=scoring,\n",
    "                random_state=RANDOM_STATE,\n",
    "                verbose=0\n",
    "            )\n",
    "            # Use thread-based parallelism to avoid pickling failures from loky processes.\n",
    "            with parallel_backend('threading'):\n",
    "                search.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "\n",
    "            results[model_name] = {\n",
    "                'best_params': search.best_params_,\n",
    "                'best_score': search.best_score_,\n",
    "                'best_model': search.best_estimator_,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "\n",
    "            print(f\"✓ Score: {search.best_score_:.4f} | Tempo: {end_time - start_time:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Erro: {str(e)[:50]}\")\n",
    "            results[model_name] = None\n",
    "    return results\n",
    "\n",
    "\n",
    "def persist_tree_model_artifacts(model_name, dataset_name, processed_data, result):\n",
    "    if result is None or result.get('best_model') is None:\n",
    "        print(f\"  ✗ {model_name} não retornou estimador treinado.\")\n",
    "        return\n",
    "\n",
    "    estimator = result['best_model']\n",
    "    X_test_encoded = processed_data['X_test_encoded']\n",
    "    y_test_encoded = processed_data['y_test_encoded']\n",
    "    label_encoder = processed_data['label_encoder']\n",
    "\n",
    "    try:\n",
    "        y_prob = estimator.predict_proba(X_test_encoded)\n",
    "    except Exception:\n",
    "        y_prob = None\n",
    "\n",
    "    y_pred = estimator.predict(X_test_encoded)\n",
    "    test_accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "    model_key = MODEL_KEY_MAP.get(model_name, model_name.lower())\n",
    "    write_artifact_bundle(\n",
    "        model_key=model_key,\n",
    "        dataset_name=dataset_name,\n",
    "        y_true=y_test_encoded,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=y_prob,\n",
    "        class_labels=label_encoder.classes_.tolist(),\n",
    "        metrics={\n",
    "            'cv_accuracy': result.get('best_score'),\n",
    "            'test_accuracy': test_accuracy,\n",
    "        },\n",
    "        hyperparams=result.get('best_params'),\n",
    "        runtime_seconds=result.get('time'),\n",
    "        extra_metadata={\n",
    "            'train_samples': len(processed_data['X_train_encoded']),\n",
    "            'test_samples': len(y_test_encoded),\n",
    "        },\n",
    "    )\n",
    "    print(f\"    → Artefatos salvos em {artifact_output_dir(model_name, dataset_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_dataset(splits):\n",
    "    X_train = splits['X_train']\n",
    "    y_train = splits['y_train']\n",
    "    X_test = splits['X_test']\n",
    "    y_test = splits['y_test']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "    categorical_cols = X_train_encoded.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if categorical_cols:\n",
    "        oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        X_train_encoded[categorical_cols] = oe.fit_transform(X_train_encoded[categorical_cols].astype(str))\n",
    "        X_test_encoded[categorical_cols] = oe.transform(X_test_encoded[categorical_cols].astype(str))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = X_train_encoded.select_dtypes(include=[np.number]).columns\n",
    "    X_train_scaled = X_train_encoded.copy()\n",
    "    X_test_scaled = X_test_encoded.copy()\n",
    "    if len(numeric_cols) > 0:\n",
    "        X_train_scaled[numeric_cols] = scaler.fit_transform(X_train_encoded[numeric_cols])\n",
    "        X_test_scaled[numeric_cols] = scaler.transform(X_test_encoded[numeric_cols])\n",
    "\n",
    "    return {\n",
    "        'X_train_encoded': X_train_encoded,\n",
    "        'X_test_encoded': X_test_encoded,\n",
    "        'X_train_scaled': X_train_scaled,\n",
    "        'X_test_scaled': X_test_scaled,\n",
    "        'y_train_encoded': y_train_encoded,\n",
    "        'y_test_encoded': y_test_encoded,\n",
    "        'y_train_original': y_train.reset_index(drop=True),\n",
    "        'y_test_original': y_test.reset_index(drop=True),\n",
    "        'label_encoder': le,\n",
    "        'categorical_cols': categorical_cols,\n",
    "        'numeric_cols': numeric_cols\n",
    "    }\n",
    "\n",
    "# Dicionários para acompanhar resultados e dados processados\n",
    "processed_datasets = {}\n",
    "total_datasets = len(train_test_splits)\n",
    "classical_artifacts = 0\n",
    "\n",
    "# Busca para modelos baseados em árvores\n",
    "for dataset_idx, (dataset_name, splits) in enumerate(train_test_splits.items(), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Dataset {dataset_idx}/{total_datasets}: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    processed_data = preprocess_dataset(splits)\n",
    "    processed_datasets[dataset_name] = processed_data\n",
    "\n",
    "    X_train_encoded = processed_data['X_train_encoded']\n",
    "    y_train_encoded = processed_data['y_train_encoded']\n",
    "\n",
    "    # Estratégia de validação cruzada\n",
    "    min_class_count = pd.Series(y_train_encoded).value_counts().min()\n",
    "    total_train_samples = len(X_train_encoded)\n",
    "    n_splits = min(CV_FOLDS, min_class_count)\n",
    "    cv_strategy = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    print(f\"\\n  Amostras: {total_train_samples} | Min classe: {min_class_count} | CV folds: {n_splits}\")\n",
    "\n",
    "    pending_models = [\n",
    "        model\n",
    "        for model in ['LightGBM', 'XGBoost', 'CatBoost']\n",
    "        if not artifact_exists(model, dataset_name)\n",
    "    ]\n",
    "\n",
    "    if not pending_models:\n",
    "        print(\"  ✓ Artefatos existentes para modelos clássicos. Nada a fazer.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n  Executando busca para: {', '.join(pending_models)}\")\n",
    "    search_results = run_classification_searches(\n",
    "        X_train=X_train_encoded,\n",
    "        y_train=y_train_encoded,\n",
    "        cv_strategy=cv_strategy,\n",
    "        target_models=pending_models,\n",
    "    )\n",
    "\n",
    "    for model_name, result in search_results.items():\n",
    "        if model_name in pending_models:\n",
    "            persist_tree_model_artifacts(model_name, dataset_name, processed_data, result)\n",
    "            if result is not None:\n",
    "                classical_artifacts += 1\n",
    "\n",
    "    print(f\"\\n  ✓ Busca concluída para modelos clássicos!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Artefatos clássicos atualizados para {classical_artifacts} treinamentos.\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666ce99",
   "metadata": {},
   "source": [
    "### Treinamento dedicado com SAINT\n",
    "Nesta etapa executamos somente a busca randômica e o re-treinamento do SAINT, separando explicitamente seu pipeline dos demais modelos. As saídas (metadados + probabilidades) são gravadas em `artifacts/saint/<dataset_slug>/` para uso posterior pelo notebook de comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb616f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_saint.config import NetworkConfig, TransformerConfig, TrainConfig, OptimizerConfig\n",
    "from lit_saint import Saint, SaintConfig, SaintDatamodule, SaintTrainer\n",
    "import torch\n",
    "\n",
    "def prepare_saint_dataframe(X, y, validation_ratio=0.2, min_val_samples=10, random_state=None):\n",
    "    \"\"\"Cria o DataFrame esperado pelo SaintDatamodule com colunas target e split.\"\"\"\n",
    "    df = X.copy().reset_index(drop=True)\n",
    "    df['target'] = y.reset_index(drop=True)\n",
    "    df['split'] = 'train'\n",
    "\n",
    "    if len(df) <= 1:\n",
    "        return df\n",
    "\n",
    "    val_size = max(int(len(df) * validation_ratio), min_val_samples)\n",
    "    if val_size >= len(df):\n",
    "        val_size = max(1, len(df) // 5) or 1\n",
    "\n",
    "    val_idx = df.sample(n=val_size, random_state=random_state).index\n",
    "    df.loc[val_idx, 'split'] = 'validation'\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_prediction_array(predictions):\n",
    "    \"\"\"Normaliza as saídas do SAINT para um array NumPy.\"\"\"\n",
    "    if isinstance(predictions, dict):\n",
    "        for key in ('predictions', 'logits', 'probs'):\n",
    "            if key in predictions:\n",
    "                predictions = predictions[key]\n",
    "                break\n",
    "        else:\n",
    "            for value in predictions.values():\n",
    "                if hasattr(value, 'shape'):\n",
    "                    predictions = value\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"Não foi possível encontrar previsões nas chaves: {list(predictions.keys())}\")\n",
    "\n",
    "    if hasattr(predictions, 'detach'):\n",
    "        predictions = predictions.detach()\n",
    "    if hasattr(predictions, 'cpu'):\n",
    "        predictions = predictions.cpu()\n",
    "    if hasattr(predictions, 'numpy'):\n",
    "        predictions = predictions.numpy()\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def saint_array_to_probabilities(preds_array):\n",
    "    tensor = torch.tensor(preds_array).float()\n",
    "    if tensor.ndim == 1 or (tensor.ndim == 2 and tensor.shape[1] == 1):\n",
    "        probs = torch.sigmoid(tensor).numpy().flatten()\n",
    "        return np.vstack([1 - probs, probs]).T\n",
    "    return torch.softmax(tensor, dim=1).numpy()\n",
    "\n",
    "\n",
    "def predictions_to_labels(predictions):\n",
    "    predictions = np.asarray(predictions)\n",
    "    if predictions.ndim == 1:\n",
    "        return (predictions > 0.5).astype(int)\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "def run_saint_random_search(processed_data, max_trials=None):\n",
    "    \"\"\"Executa busca randômica de hiperparâmetros e reentreina o SAINT com os melhores valores.\"\"\"\n",
    "    start_time = time.time()\n",
    "    trials = max_trials or SEARCH_ITERATIONS\n",
    "    X_train_features = processed_data['X_train_scaled']\n",
    "    y_train_raw = processed_data['y_train_original']\n",
    "    y_train_encoded = processed_data['y_train_encoded']\n",
    "    X_test_features = processed_data['X_test_scaled']\n",
    "    y_test_encoded = processed_data['y_test_encoded']\n",
    "    label_encoder = processed_data['label_encoder']\n",
    "\n",
    "    df_train_full = prepare_saint_dataframe(\n",
    "        X=X_train_features,\n",
    "        y=y_train_raw,\n",
    "        validation_ratio=0.2,\n",
    "        min_val_samples=10,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    n_classes = len(np.unique(y_train_encoded))\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    print(\"  SAINT (Random Search)...\", end=' ', flush=True)\n",
    "\n",
    "    for trial_idx in range(trials):\n",
    "        params = {\n",
    "            'depth': int(param_spaces_classifier['Saint']['depth'].rvs()),\n",
    "            'heads': int(param_spaces_classifier['Saint']['heads'].rvs()),\n",
    "            'dropout': float(param_spaces_classifier['Saint']['dropout'].rvs()),\n",
    "            'learning_rate': float(param_spaces_classifier['Saint']['learning_rate'].rvs()),\n",
    "            'batch_size': int(param_spaces_classifier['Saint']['batch_size'].rvs()),\n",
    "            'epochs': int(param_spaces_classifier['Saint']['epochs'].rvs())\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            data_module = SaintDatamodule(\n",
    "                df=df_train_full,\n",
    "                target='target',\n",
    "                split_column='split'\n",
    "            )\n",
    "\n",
    "            saint_config = SaintConfig(\n",
    "                network=NetworkConfig(\n",
    "                    transformer=TransformerConfig(\n",
    "                        depth=params['depth'],\n",
    "                        heads=params['heads'],\n",
    "                        dropout=params['dropout']\n",
    "                    )\n",
    "                ),\n",
    "                train=TrainConfig(\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    optimizer=OptimizerConfig(learning_rate=params['learning_rate'])\n",
    "                )\n",
    "            )\n",
    "\n",
    "            saint_model = Saint(\n",
    "                categories=data_module.categorical_dims,\n",
    "                continuous=data_module.numerical_columns,\n",
    "                config=saint_config,\n",
    "                dim_target=n_classes\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                max_epochs=params['epochs'],\n",
    "                enable_progress_bar=False,\n",
    "                logger=False,\n",
    "                enable_model_summary=False\n",
    "            )\n",
    "            saint_trainer = SaintTrainer(trainer=trainer)\n",
    "            saint_trainer.fit(model=saint_model, datamodule=data_module, enable_pretraining=False)\n",
    "\n",
    "            df_val = df_train_full[df_train_full['split'] == 'validation'].copy()\n",
    "            if df_val.empty:\n",
    "                continue\n",
    "\n",
    "            predictions = saint_trainer.predict(\n",
    "                model=saint_model,\n",
    "                df=df_val.drop(columns=['target', 'split']),\n",
    "                datamodule=data_module\n",
    "            )\n",
    "            preds_array = extract_prediction_array(predictions)\n",
    "            y_pred = predictions_to_labels(preds_array)\n",
    "            y_true = label_encoder.transform(df_val['target'].values)\n",
    "            score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "        except Exception as e:\n",
    "            print(f\"\\n    Erro SAINT (tentativa {trial_idx + 1}): {str(e)[:80]}\")\n",
    "\n",
    "    if best_params is None:\n",
    "        print(\"✗ Não foi possível treinar SAINT.\")\n",
    "        return None\n",
    "\n",
    "    # Reentreina utilizando todo o conjunto de treino, mantendo um pequeno holdout para o datamodule\n",
    "    df_train_final = prepare_saint_dataframe(\n",
    "        X=X_train_features,\n",
    "        y=y_train_raw,\n",
    "        validation_ratio=0.05,\n",
    "        min_val_samples=1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    data_module = SaintDatamodule(\n",
    "        df=df_train_final,\n",
    "        target='target',\n",
    "        split_column='split'\n",
    "    )\n",
    "\n",
    "    saint_config = SaintConfig(\n",
    "        network=NetworkConfig(\n",
    "            transformer=TransformerConfig(\n",
    "                depth=best_params['depth'],\n",
    "                heads=best_params['heads'],\n",
    "                dropout=best_params['dropout']\n",
    "            )\n",
    "        ),\n",
    "        train=TrainConfig(\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            optimizer=OptimizerConfig(learning_rate=best_params['learning_rate'])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    saint_model = Saint(\n",
    "        categories=data_module.categorical_dims,\n",
    "        continuous=data_module.numerical_columns,\n",
    "        config=saint_config,\n",
    "        dim_target=n_classes\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=best_params['epochs'],\n",
    "        enable_progress_bar=False,\n",
    "        logger=False,\n",
    "        enable_model_summary=False\n",
    "    )\n",
    "    saint_trainer = SaintTrainer(trainer=trainer)\n",
    "    saint_trainer.fit(model=saint_model, datamodule=data_module, enable_pretraining=False)\n",
    "\n",
    "    df_test = X_test_features.copy().reset_index(drop=True)\n",
    "    predictions = saint_trainer.predict(model=saint_model, df=df_test, datamodule=data_module)\n",
    "    preds_array = extract_prediction_array(predictions)\n",
    "    y_prob_test = saint_array_to_probabilities(preds_array)\n",
    "    y_pred_test = predictions_to_labels(preds_array)\n",
    "    test_score = accuracy_score(y_test_encoded, y_pred_test)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f\"✓ Melhor score: {best_score:.4f} | Teste: {test_score:.4f} | Tempo: {total_time:.2f}s\")\n",
    "    \n",
    "    # Retorna apenas as informações necessárias para gerar os artefatos\n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'time': total_time,\n",
    "        'y_pred': y_pred_test,\n",
    "        'y_prob': y_prob_test,\n",
    "        'label_encoder': label_encoder,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab412b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop exclusivo para SAINT\n",
    "saint_datasets_processed = 0\n",
    "for dataset_idx, (dataset_name, splits) in enumerate(train_test_splits.items(), 1):\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"SAINT | Dataset {dataset_idx}/{len(train_test_splits)}: {dataset_name}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    if artifact_exists('SAINT', dataset_name):\n",
    "        print(\"  ✓ Artefatos SAINT já existem. Pulando...\")\n",
    "        continue\n",
    "\n",
    "    processed_data = processed_datasets.get(dataset_name)\n",
    "    if processed_data is None:\n",
    "        processed_data = preprocess_dataset(splits)\n",
    "        processed_datasets[dataset_name] = processed_data\n",
    "\n",
    "    saint_result = run_saint_random_search(processed_data)\n",
    "\n",
    "    if saint_result is None:\n",
    "        print(\"  ✗ SAINT não treinado.\")\n",
    "        continue\n",
    "\n",
    "    write_artifact_bundle(\n",
    "        model_key=MODEL_KEY_MAP['SAINT'],\n",
    "        dataset_name=dataset_name,\n",
    "        y_true=processed_data['y_test_encoded'],\n",
    "        y_pred=saint_result['y_pred'],\n",
    "        y_prob=saint_result['y_prob'],\n",
    "        class_labels=processed_data['label_encoder'].classes_.tolist(),\n",
    "        metrics={\n",
    "            'cv_accuracy': saint_result['best_score'],\n",
    "            'test_accuracy': saint_result['test_score'],\n",
    "        },\n",
    "        hyperparams=saint_result['best_params'],\n",
    "        runtime_seconds=saint_result['time'],\n",
    "        extra_metadata={\n",
    "            'train_samples': len(processed_data['X_train_encoded']),\n",
    "            'test_samples': len(processed_data['X_test_encoded']),\n",
    "            'search_iterations': SEARCH_ITERATIONS,\n",
    "        },\n",
    "    )\n",
    "    print(f\"  ✓ Artefatos SAINT salvos em {artifact_output_dir('SAINT', dataset_name)}\")\n",
    "    saint_datasets_processed += 1\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(f\"SAINT executado para {saint_datasets_processed} datasets.\")\n",
    "print(f\"{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb89c9",
   "metadata": {},
   "source": [
    "### Treinamento dedicado com AutoGluon\n",
    "Nesta etapa executamos o AutoGluon AutoML separadamente, utilizando validação automática e ensemble de modelos. Assim como nos demais blocos, os resultados são persistidos em `artifacts/autogluon/<dataset_slug>/` para consumo offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import shutil\n",
    "\n",
    "autogluon_datasets_processed = 0\n",
    "total_datasets = len(train_test_splits)\n",
    "for dataset_idx, (dataset_name, splits) in enumerate(train_test_splits.items(), 1):\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"AutoGluon | Dataset {dataset_idx}/{total_datasets}: {dataset_name}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    if artifact_exists('AutoGluon', dataset_name):\n",
    "        print(\"  ✓ Artefatos AutoGluon já existem. Pulando...\")\n",
    "        continue\n",
    "\n",
    "    processed_data = processed_datasets.get(dataset_name)\n",
    "    if processed_data is None:\n",
    "        processed_data = preprocess_dataset(splits)\n",
    "        processed_datasets[dataset_name] = processed_data\n",
    "\n",
    "    X_train_encoded = processed_data['X_train_encoded']\n",
    "    y_train_encoded = processed_data['y_train_encoded']\n",
    "    X_test_encoded = processed_data['X_test_encoded']\n",
    "    y_test_encoded = processed_data['y_test_encoded']\n",
    "\n",
    "    print(f\"  AutoGluon...\", end=' ', flush=True)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        ag_dir = get_dataset_dir(MODEL_KEY_MAP['AutoGluon'], dataset_name) / 'autogluon_internal'\n",
    "        if ag_dir.exists():\n",
    "            shutil.rmtree(ag_dir)\n",
    "        ag_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        train_data = X_train_encoded.copy()\n",
    "        train_data['target'] = y_train_encoded\n",
    "\n",
    "        test_data = X_test_encoded.copy()\n",
    "        test_data['target'] = y_test_encoded\n",
    "\n",
    "        predictor = TabularPredictor(\n",
    "            label='target',\n",
    "            problem_type='multiclass',\n",
    "            eval_metric='accuracy',\n",
    "            path=str(ag_dir)\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            train_data,\n",
    "            time_limit=300,\n",
    "            presets='best_quality'\n",
    "        )\n",
    "\n",
    "        leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "        best_score_ag = leaderboard.iloc[0].get('score_test')\n",
    "        if best_score_ag is None:\n",
    "            best_score_ag = leaderboard.iloc[0].get('score_val')\n",
    "        best_score_ag = float(best_score_ag) if best_score_ag is not None else np.nan\n",
    "        \n",
    "        y_prob = predictor.predict_proba(X_test_encoded).to_numpy()\n",
    "        y_pred = predictor.predict(X_test_encoded)\n",
    "        test_accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        write_artifact_bundle(\n",
    "            model_key=MODEL_KEY_MAP['AutoGluon'],\n",
    "            dataset_name=dataset_name,\n",
    "            y_true=y_test_encoded,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            class_labels=processed_data['label_encoder'].classes_.tolist(),\n",
    "            metrics={\n",
    "                'leaderboard_score': best_score_ag,\n",
    "                'test_accuracy': test_accuracy,\n",
    "            },\n",
    "            hyperparams={'time_limit': 300, 'presets': 'best_quality'},\n",
    "            runtime_seconds=total_time,\n",
    "            extra_metadata={\n",
    "                'train_samples': len(X_train_encoded),\n",
    "                'test_samples': len(y_test_encoded),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Score Eval: {best_score_ag:.4f} | Test: {test_accuracy:.4f} | Tempo: {total_time:.2f}s\")\n",
    "        print(f\"    → Artefatos salvos em {artifact_output_dir('AutoGluon', dataset_name)}\")\n",
    "        autogluon_datasets_processed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Erro: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(f\"AutoGluon executado para {autogluon_datasets_processed} datasets.\")\n",
    "print(f\"{'-'*80}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
